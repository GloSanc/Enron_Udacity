{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier()\n",
      "Pipeline(memory=None,\n",
      "         steps=[('rob_sc',\n",
      "                 RobustScaler(copy=True, quantile_range=(25.0, 75.0),\n",
      "                              with_centering=True, with_scaling=True)),\n",
      "                ['clf',\n",
      "                 RandomForestClassifier(bootstrap=True, class_weight=None,\n",
      "                                        criterion='gini', max_depth=None,\n",
      "                                        max_features='auto',\n",
      "                                        max_leaf_nodes=None,\n",
      "                                        min_impurity_decrease=0.0,\n",
      "                                        min_impurity_split=None,\n",
      "                                        min_samples_leaf=1, min_samples_split=2,\n",
      "                                        min_weight_fraction_leaf=0.0,\n",
      "                                        n_estimators=10, n_jobs=None,\n",
      "                                        oob_score=False, random_state=None,\n",
      "                                        verbose=0, warm_start=False)]],\n",
      "         verbose=False)\n",
      "\tAccuracy: 0.85813\tPrecision: 0.39577\tRecall: 0.12150\tF1: 0.18592\tF2: 0.14105\n",
      "\tTotal predictions: 15000\tTrue positives:  243\tFalse positives:  371\tFalse negatives: 1757\tTrue negatives: 12629\n",
      "\n",
      "AdaBoostClassifier()\n",
      "Pipeline(memory=None,\n",
      "         steps=[('rob_sc',\n",
      "                 RobustScaler(copy=True, quantile_range=(25.0, 75.0),\n",
      "                              with_centering=True, with_scaling=True)),\n",
      "                ['clf',\n",
      "                 AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
      "                                    learning_rate=1.0, n_estimators=50,\n",
      "                                    random_state=None)]],\n",
      "         verbose=False)\n",
      "\tAccuracy: 0.87247\tPrecision: 0.53083\tRecall: 0.37450\tF1: 0.43917\tF2: 0.39794\n",
      "\tTotal predictions: 15000\tTrue positives:  749\tFalse positives:  662\tFalse negatives: 1251\tTrue negatives: 12338\n",
      "\n",
      "RandomForestClassifier(finetune)\n",
      "Pipeline(memory=None,\n",
      "         steps=[('rob_sc',\n",
      "                 RobustScaler(copy=True, quantile_range=(25.0, 75.0),\n",
      "                              with_centering=True, with_scaling=True)),\n",
      "                ['clf',\n",
      "                 RandomForestClassifier(bootstrap=True, class_weight=None,\n",
      "                                        criterion='gini', max_depth=7,\n",
      "                                        max_features=2, max_leaf_nodes=None,\n",
      "                                        min_impurity_decrease=0.0,\n",
      "                                        min_impurity_split=None,\n",
      "                                        min_samples_leaf=1, min_samples_split=2,\n",
      "                                        min_weight_fraction_leaf=0.0,\n",
      "                                        n_estimators=200, n_jobs=None,\n",
      "                                        oob_score=False, random_state=42,\n",
      "                                        verbose=0, warm_start=False)]],\n",
      "         verbose=False)\n",
      "\tAccuracy: 0.87693\tPrecision: 0.57505\tRecall: 0.29500\tF1: 0.38995\tF2: 0.32683\n",
      "\tTotal predictions: 15000\tTrue positives:  590\tFalse positives:  436\tFalse negatives: 1410\tTrue negatives: 12564\n",
      "\n",
      "AdaBoostClassifier(finetune)\n",
      "Pipeline(memory=None,\n",
      "         steps=[('rob_sc',\n",
      "                 RobustScaler(copy=True, quantile_range=(25.0, 75.0),\n",
      "                              with_centering=True, with_scaling=True)),\n",
      "                ['clf',\n",
      "                 AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
      "                                    learning_rate=1.5, n_estimators=500,\n",
      "                                    random_state=42)]],\n",
      "         verbose=False)\n",
      "\tAccuracy: 0.89253\tPrecision: 0.65696\tRecall: 0.40600\tF1: 0.50185\tF2: 0.43958\n",
      "\tTotal predictions: 15000\tTrue positives:  812\tFalse positives:  424\tFalse negatives: 1188\tTrue negatives: 12576\n",
      "\n",
      "AdaBoostClassifier(base estimator RandonForest and finetune)\n",
      "Pipeline(memory=None,\n",
      "         steps=[('rob_sc',\n",
      "                 RobustScaler(copy=True, quantile_range=(25.0, 75.0),\n",
      "                              with_centering=True, with_scaling=True)),\n",
      "                ['clf',\n",
      "                 AdaBoostClassifier(algorithm='SAMME.R',\n",
      "                                    base_estimator=RandomForestClassifier(bootstrap=True,\n",
      "                                                                          class_weight=None,\n",
      "                                                                          criterion='gini',\n",
      "                                                                          max_depth=7,\n",
      "                                                                          max_features=2,\n",
      "                                                                          max_leaf_nodes=None,\n",
      "                                                                          min_impurity_decrease=0.0,\n",
      "                                                                          min_impurity_split=None,\n",
      "                                                                          min_samples_leaf=1,\n",
      "                                                                          min_samples_split=2,\n",
      "                                                                          min_weight_fraction_leaf=0.0,\n",
      "                                                                          n_estimators=200,\n",
      "                                                                          n_jobs=None,\n",
      "                                                                          oob_score=False,\n",
      "                                                                          random_state=42,\n",
      "                                                                          verbose=0,\n",
      "                                                                          warm_start=False),\n",
      "                                    learning_rate=1.5, n_estimators=500,\n",
      "                                    random_state=42)]],\n",
      "         verbose=False)\n",
      "\tAccuracy: 0.87767\tPrecision: 0.58144\tRecall: 0.29450\tF1: 0.39097\tF2: 0.32675\n",
      "\tTotal predictions: 15000\tTrue positives:  589\tFalse positives:  424\tFalse negatives: 1411\tTrue negatives: 12576\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    " \n",
    "\n",
    "# Import needed along the file\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../tools/\")\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score \n",
    "from sklearn.metrics import classification_report, f1_score, make_scorer\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\n",
    "\n",
    "\n",
    "\n",
    "# Define Function to correct data, remove outlier and define new features\n",
    "### Task 1: Select what features to use.\n",
    "### The first feature must be \"poi\".\n",
    "### Load the dictionary containing the dataset\n",
    "### Task 2: Remove outliers\n",
    "### Task 3: Create new feature(s)\n",
    "### Store to my_dataset for easy export below.\n",
    "\n",
    "\n",
    "\n",
    "# Function to do taks 1, 2 and 3\n",
    "def create_df (features_original_list,features_amendm_list):\n",
    "    \n",
    "    \n",
    "    import pickle\n",
    "    from feature_format import featureFormat, targetFeatureSplit\n",
    "    #from tester import dump_classifier_and_data\n",
    "    from sklearn.decomposition import PCA\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    import warnings\n",
    "    warnings.filterwarnings('ignore')\n",
    "    \n",
    "##### Loading the dictionary containing the dataset\n",
    "\n",
    "    data_dict = pickle.load(open(\"final_project_dataset.pkl\", \"rb\"))\n",
    "   \n",
    "    \n",
    "##### Right data to be corrected in the dataset\n",
    "\n",
    "    data_dict['BELFER ROBERT']={'salary': 'NaN',\n",
    "     'to_messages': 'NaN',\n",
    "     'deferral_payments': 'NaN',\n",
    "     'total_payments': 3285,\n",
    "     'loan_advances': 'NaN',\n",
    "     'bonus': 'NaN',\n",
    "     'email_address': 'NaN',\n",
    "     'restricted_stock_deferred': -44093,\n",
    "     'deferred_income':  -102500,\n",
    "     'total_stock_value': 'NaN',\n",
    "     'expenses': 3285,\n",
    "     'from_poi_to_this_person': 'NaN',\n",
    "     'exercised_stock_options': 'NaN',\n",
    "     'from_messages': 'NaN',\n",
    "     'other': 'NaN',\n",
    "     'from_this_person_to_poi': 'NaN',\n",
    "     'poi': False,\n",
    "     'long_term_incentive': 'NaN',\n",
    "     'shared_receipt_with_poi': 'NaN',\n",
    "     'restricted_stock': 44093,\n",
    "     'director_fees': 102500}\n",
    "\n",
    "    data_dict['BHATNAGAR SANJAY']= {'salary': 'NaN',\n",
    "     'to_messages': 523,\n",
    "     'total_stock_value': 15456290,\n",
    "     'deferral_payments': 'NaN',\n",
    "     'total_payments': 137864,\n",
    "     'loan_advances': 'NaN',\n",
    "     'bonus': 'NaN',\n",
    "     'email_address': 'sanjay.bhatnagar@enron.com',\n",
    "     'restricted_stock_deferred': -2604490,\n",
    "     'deferred_income': 'NaN',\n",
    "     'expenses': 137864,\n",
    "     'from_poi_to_this_person': 0,\n",
    "     'exercised_stock_options': 15456290,\n",
    "     'from_messages': 29,\n",
    "     'other': 'NaN',\n",
    "     'from_this_person_to_poi': 1,\n",
    "     'poi': False,\n",
    "     'long_term_incentive': 'NaN',\n",
    "     'shared_receipt_with_poi': 463,\n",
    "     'restricted_stock': 2604490,\n",
    "     'director_fees': 'NaN'}\n",
    "    \n",
    "    \n",
    "##### Entries to be deleted in the dataset\n",
    "    list_out=['TOTAL','THE TRAVEL AGENCY IN THE PARK']\n",
    "    \n",
    "##### Deleting the entries in the dictionary. \n",
    "    for name in list_out:\n",
    "        data_dict.pop(name,0)\n",
    "        \n",
    "        \n",
    "##### New features\n",
    "\n",
    "    for v in data_dict.values():\n",
    "        salary=v[\"salary\"]\n",
    "        bonus = v[\"bonus\"]\n",
    "        expenses = v[\"expenses\"]\n",
    "        other = v[\"other\"]\n",
    "        \n",
    "        long_term_incentive = v[\"long_term_incentive\"]\n",
    "        exercised_stock_options =v[\"exercised_stock_options\"] \n",
    "        \n",
    "        total_payments = v[\"total_payments\"]\n",
    "        total_stock_value = v[\"total_stock_value\"]\n",
    "\n",
    "        shared_receipt_with_poi = v[\"shared_receipt_with_poi\"]\n",
    "        from_poi_to_this_person = v[\"from_poi_to_this_person\"]\n",
    "        from_this_person_to_poi = v[\"from_this_person_to_poi\"]\n",
    " \n",
    "                     \n",
    "        v[\"incentives\"] =  (float(  bonus) + float( long_term_incentive) + float(  exercised_stock_options) if\n",
    "                            bonus not in [0, \"NaN\"] and long_term_incentive not in [0, \"NaN\"] \n",
    "                            and  exercised_stock_options not in [0, \"NaN\"]  else 0.0)\n",
    "        \n",
    "        incentives= v[\"incentives\"] \n",
    "            \n",
    "        v[\"total_money\"] =  (float( total_payments) + float( total_stock_value) if\n",
    "                            total_payments not in [0, \"NaN\"] and total_stock_value\n",
    "                           not in [0, \"NaN\"] else 0.0)\n",
    "        \n",
    "        total_money=  v[\"total_money\"]  \n",
    "        \n",
    "\n",
    "        v[\"incentives_ratio\"] =  (float(  incentives) / float( total_money) if\n",
    "                          incentives not in [0, \"NaN\"] and total_money\n",
    "                           not in [0, \"NaN\"] else 0.0)\n",
    "                \n",
    "        incentives_ratio= v[\"incentives_ratio\"] \n",
    "        \n",
    "        \n",
    "        v[\"from_this_person_to_poi_ratio\"] =  (float( from_this_person_to_poi) / float( shared_receipt_with_poi) if\n",
    "                         from_this_person_to_poi not in [0, \"NaN\"] and shared_receipt_with_poi\n",
    "                           not in [0, \"NaN\"] else 0.0)\n",
    "                \n",
    "        from_this_person_to_poi_ratio= v[\"from_this_person_to_poi_ratio\"] \n",
    "              \n",
    "             \n",
    "    features_original_list.append(\"incentives\") \n",
    "    features_original_list.append(\"incentives_ratio\")\n",
    "    features_original_list.append(\"total_money\")                    \n",
    "    features_original_list.append(\"from_this_person_to_poi_ratio\")     \n",
    "    \n",
    "    \n",
    "\n",
    "    my_dataset = data_dict\n",
    "\n",
    "##### data frame\n",
    "    data_df = pd.DataFrame.from_dict(data_dict, orient='index')\n",
    "    data_df= data_df.loc[ :,(features_amendm_list )]  \n",
    "\n",
    "    data_df= data_df.replace('NaN', 0.0)\n",
    "    data_df.round(2)\n",
    "\n",
    "    #print('data frame shape',data_df.shape)\n",
    "    return data_df\n",
    "\n",
    "\n",
    "\n",
    "# Run Function to correct data, remove outlier and define new features\n",
    "##### 1. original features\n",
    "features_list0=[ 'poi',\n",
    "                               'salary', 'bonus', 'long_term_incentive',\n",
    "                               'deferred_income','deferral_payments', 'loan_advances',\n",
    "                               'other', 'expenses','director_fees',  'total_payments',\n",
    "                               'exercised_stock_options', 'restricted_stock',\n",
    "                               'restricted_stock_deferred', 'total_stock_value',\n",
    "                               'from_messages', 'to_messages', \n",
    "                               'from_poi_to_this_person', 'from_this_person_to_poi','shared_receipt_with_poi']\n",
    "\n",
    "##### 2. features including new ones and the ones used for new features creation and for algorithm \n",
    "features_list1=[ 'poi',\n",
    "                 'salary', 'bonus', 'long_term_incentive',\n",
    "                 'deferred_income','deferral_payments',\n",
    "                 'other', 'expenses','total_payments',\n",
    "                 'exercised_stock_options', 'restricted_stock', 'total_stock_value',\n",
    "                 'from_messages', 'to_messages', \n",
    "                 'from_poi_to_this_person', 'from_this_person_to_poi', 'shared_receipt_with_poi',\n",
    "               \n",
    "               \"incentives_ratio\", \"from_this_person_to_poi_ratio\",\n",
    "               \"incentives\",\"total_money\"   , 'payment_2', 'payment_f'\n",
    "             ]\n",
    "\n",
    "##### 3. run the function \n",
    "data_df=create_df (features_list0,features_list1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Features creation with PCA\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "        \n",
    "payment_f=['salary', 'bonus', 'long_term_incentive', 'other', 'expenses']\n",
    "payment_2=['salary', 'other', 'expenses']\n",
    "\n",
    "pca = PCA(n_components=1)\n",
    "pca.fit(data_df[payment_2])\n",
    "pcaComponents = pca.fit_transform(data_df[payment_2])\n",
    "\n",
    "data_df['payment_2']=pcaComponents\n",
    "\n",
    "pca = PCA(n_components=1)\n",
    "pca.fit(data_df[payment_f])\n",
    "pcaComponents = pca.fit_transform(data_df[payment_f])\n",
    "\n",
    "data_df['payment_f']=pcaComponents\n",
    "\n",
    "# create my_dataset with the features to be used\n",
    "data_df  = data_df .loc[ :,( features_list1)]\n",
    "\n",
    "data_df.round(2)\n",
    "\n",
    "features_list=[ 'poi','bonus','other', 'expenses','total_payments',\n",
    "                 'exercised_stock_options', 'total_stock_value',\n",
    "               'from_this_person_to_poi',\n",
    "              \"incentives_ratio\", \"from_this_person_to_poi_ratio\",\n",
    "               \"incentives\",\"total_money\" , 'payment_f', 'payment_2'  ]\n",
    "\n",
    "data_df  = data_df .loc[ :,( features_list)]\n",
    "data_dict=data_df.to_dict('index')\n",
    "my_dataset=data_dict\n",
    "\n",
    "### Task 4: Try a varity of classifiers\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Definiton of different classifier to test     \n",
    "# 0. RandomForest and Ada Boost basic clasifier\n",
    "clf0_1=RandomForestClassifier()\n",
    "clf0_2=AdaBoostClassifier()\n",
    "\n",
    "### Task 5: Tune your classifier to achieve better than .3 precision and recall\n",
    "### using our testing script. Check the tester.py script in the final project\n",
    "### folder for details on the evaluation method, especially the test_classifier\n",
    "### function. \n",
    "#1. RandomForest finetune with some features\n",
    "clf_rf2=RandomForestClassifier(n_estimators=200, max_depth=7, max_features = 2, random_state=42)\n",
    "features_rfd=['poi', 'bonus','exercised_stock_options', 'total_stock_value', 'expenses' ,\n",
    "              'total_payments','incentives_ratio'  ]\n",
    "\n",
    "# 2. Ada Boost finetune with some features\n",
    "clf_ada2=AdaBoostClassifier(base_estimator=None, n_estimators=500, learning_rate=1.5, random_state=42)\n",
    "features_adah=['poi','exercised_stock_options','other','expenses','total_money','incentives_ratio', \n",
    "                'from_this_person_to_poi',\n",
    "               'total_stock_value','payment_f', \"incentives\", 'payment_2']\n",
    "\n",
    "\n",
    "# 3. Ada Boost finetune with RadomForest base estimator with some features\n",
    "clf_rf2=RandomForestClassifier(n_estimators=200, max_depth=7, max_features = 2, random_state=42)\n",
    "clf_ada2_rf2=AdaBoostClassifier(base_estimator=clf_rf2, n_estimators=500, learning_rate=1.5, random_state=42)\n",
    "features_rfd=['poi', 'bonus','exercised_stock_options', 'total_stock_value', 'expenses' ,\n",
    "              'total_payments','incentives_ratio'  ]\n",
    "\n",
    "\n",
    "PERF_FORMAT_STRING = \"\\\n",
    "\\tAccuracy: {:>0.{display_precision}f}\\tPrecision: {:>0.{display_precision}f}\\t\\\n",
    "Recall: {:>0.{display_precision}f}\\tF1: {:>0.{display_precision}f}\\tF2: {:>0.{display_precision}f}\"\n",
    "RESULTS_FORMAT_STRING = \"\\tTotal predictions: {:4d}\\tTrue positives: {:4d}\\tFalse positives: {:4d}\\\n",
    "\\tFalse negatives: {:4d}\\tTrue negatives: {:4d}\"\n",
    "\n",
    "\n",
    "# Define tester: this is the one provided in tester.py\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "def test_classifier(clf, dataset, feature_list, folds=1000):\n",
    "    data = featureFormat(dataset, feature_list, sort_keys=True)\n",
    "    labels, features = targetFeatureSplit(data)\n",
    "   # cv = StratifiedShuffleSplit(labels, folds, random_state=42)\n",
    "    cv = StratifiedShuffleSplit(n_splits=folds, random_state=42)\n",
    "    true_negatives = 0\n",
    "    false_negatives = 0\n",
    "    true_positives = 0\n",
    "    false_positives = 0\n",
    "    #for train_idx, test_idx in cv:\n",
    "    for train_idx, test_idx in cv.split (features, labels):\n",
    "        features_train = []\n",
    "        features_test = []\n",
    "        labels_train = []\n",
    "        labels_test = []\n",
    "        for ii in train_idx:\n",
    "            features_train.append(features[ii])\n",
    "            labels_train.append(labels[ii])\n",
    "        for jj in test_idx:\n",
    "            features_test.append(features[jj])\n",
    "            labels_test.append(labels[jj])\n",
    "\n",
    "        ### fit the classifier using training set, and test on test set\n",
    "        clf.fit(features_train, labels_train)\n",
    "        predictions = clf.predict(features_test)\n",
    "        for prediction, truth in zip(predictions, labels_test):\n",
    "            if prediction == 0 and truth == 0:\n",
    "                true_negatives += 1\n",
    "            elif prediction == 0 and truth == 1:\n",
    "                false_negatives += 1\n",
    "            elif prediction == 1 and truth == 0:\n",
    "                false_positives += 1\n",
    "            elif prediction == 1 and truth == 1:\n",
    "                true_positives += 1\n",
    "            else:\n",
    "                print(\"Warning: Found a predicted label not == 0 or 1.\")\n",
    "                print(\"All predictions should take value 0 or 1.\")\n",
    "                print(\"Evaluating performance for processed predictions:\")\n",
    "                break\n",
    "    try:\n",
    "        total_predictions = true_negatives + false_negatives + false_positives + true_positives\n",
    "        accuracy = 1.0*(true_positives + true_negatives)/total_predictions\n",
    "        precision = 1.0*true_positives/(true_positives+false_positives)\n",
    "        recall = 1.0*true_positives/(true_positives+false_negatives)\n",
    "        f1 = 2.0 * true_positives/(2*true_positives + false_positives+false_negatives)\n",
    "        f2 = (1+2.0*2.0) * precision*recall/(4*precision + recall)\n",
    "        print(clf)\n",
    "        print(PERF_FORMAT_STRING.format(accuracy, precision, recall, f1, f2, display_precision = 5))\n",
    "        print(RESULTS_FORMAT_STRING.format(total_predictions, true_positives, false_positives, \n",
    "                                           false_negatives, true_negatives))\n",
    "        print(\"\")\n",
    "\n",
    "               \n",
    "    except:\n",
    "        print(\"Got a divide by zero when trying out:\", clf)\n",
    "\n",
    "# Define pipeline for scaler and classifier\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "clf_trial=clf0_1\n",
    "pipe_clf= Pipeline([['rob_sc',RobustScaler() ],['clf', clf_trial]])\n",
    "clf_0_1p=pipe_clf   \n",
    "# Test the classifier\n",
    "print('RandomForestClassifier()')\n",
    "test_classifier(clf_0_1p, my_dataset, features_list, folds=1000)\n",
    "\n",
    "# Define pipeline for scaler and classifier\n",
    "clf_trial=clf0_2\n",
    "pipe_clf= Pipeline([['rob_sc',RobustScaler() ],['clf', clf_trial]])\n",
    "clf_0_2p=pipe_clf\n",
    "# Test the classifier\n",
    "print('AdaBoostClassifier()')\n",
    "test_classifier(clf_0_2p, my_dataset, features_list, folds=1000)\n",
    "\n",
    "# Define pipeline for scaler and classifier\n",
    "clf_trial=clf_rf2\n",
    "pipe_clf= Pipeline([['rob_sc',RobustScaler() ],['clf', clf_trial]])\n",
    "clf_rf2p=pipe_clf\n",
    "# Test the classifier\n",
    "print('RandomForestClassifier(finetune)')\n",
    "test_classifier(clf_rf2p, my_dataset, features_rfd, folds=1000)\n",
    "\n",
    "# Define pipeline for scaler and classifier\n",
    "clf_trial=clf_ada2\n",
    "pipe_clf= Pipeline([['rob_sc',RobustScaler() ],['clf', clf_trial]])\n",
    "clf_ada2p=pipe_clf\n",
    "# Test the classifier\n",
    "print('AdaBoostClassifier(finetune)')\n",
    "test_classifier(clf_ada2p, my_dataset, features_adah, folds=1000)\n",
    "\n",
    "# Define pipeline for scaler and classifier\n",
    "clf_trial=clf_ada2_rf2\n",
    "pipe_clf= Pipeline([['rob_sc',RobustScaler() ],['clf', clf_trial]])\n",
    "clf_ada2_rf2p=pipe_clf\n",
    "# Test the classifier\n",
    "print('AdaBoostClassifier(base estimator RandonForest and finetune)')\n",
    "test_classifier(clf_ada2_rf2p, my_dataset, features_rfd, folds=1000)\n",
    "\n",
    "### Task 6: Dump your classifier, dataset, and features_list so anyone can\n",
    "### check your results. You do not need to change anything below, but make sure\n",
    "### that the version of poi_id.py that you submit can be run on its own and\n",
    "### generates the necessary .pkl files for validating your results.\n",
    "\n",
    "CLF_PICKLE_FILENAME = \"my_classifier.pkl\"\n",
    "DATASET_PICKLE_FILENAME = \"my_dataset.pkl\"\n",
    "FEATURE_LIST_FILENAME = \"my_feature_list.pkl\"\n",
    "\n",
    "def dump_classifier_and_data(clf, dataset, feature_list):\n",
    "    with open(CLF_PICKLE_FILENAME, \"wb\") as clf_outfile:\n",
    "        pickle.dump(clf, clf_outfile)\n",
    "    with open(DATASET_PICKLE_FILENAME, \"wb\") as dataset_outfile:\n",
    "        pickle.dump(dataset, dataset_outfile)\n",
    "    with open(FEATURE_LIST_FILENAME, \"wb\") as featurelist_outfile:\n",
    "        pickle.dump(feature_list, featurelist_outfile)\n",
    "\n",
    "def load_classifier_and_data():\n",
    "    with open(CLF_PICKLE_FILENAME, \"rb\") as clf_infile:\n",
    "        clf = pickle.load(clf_infile)\n",
    "    with open(DATASET_PICKLE_FILENAME, \"rb\") as dataset_infile:\n",
    "        dataset = pickle.load(dataset_infile)\n",
    "    with open(FEATURE_LIST_FILENAME, \"rb\") as featurelist_infile:\n",
    "        feature_list = pickle.load(featurelist_infile)\n",
    "    return clf, dataset, feature_list\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "clf=clf_ada2\n",
    "my_dataset=data_dict\n",
    "features_list=features_adah\n",
    "\n",
    "dump_classifier_and_data(clf, my_dataset, features_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
